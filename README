

				 Wikiprep
			      ==============
			       Zemanta fork

	  MediaWiki syntax preprocessor and information extractor



Content
=======

1.	Introduction
1.1	  Relation to the original Wikiprep
1.2	  Current status

2.	Requirements and installation
2.1	  Software
2.2	  Hardware

3.	Usage

4.	Tools

5.	License

6.	Hacking



1. Introduction
===============

Wikiprep is a Perl script that parses MediaWiki data dumps in XML format
and extracts useful information from them (MediaWiki is the software that
is best known for running Wikipedia and other Wikimedia foundation
projects).

MediaWiki uses a markup language (wiki syntax) that is optimized for easy
editing by human beings. It contains a lot of quirks, special cases and 
odd corners that help MediaWiki correctly display wiki pages, even when
they contain typing errors. This makes parsing this syntax with other
software highly non-trivial.

One goal of Wikiprep is to implement a parser that:
  
  o is compatible with MediaWiki as closely as possible, implementing as
    much functionality as is needed to achive other goals.

  o is as fast as possible, allowing tracking the English Wikipedia dataset
    as closely as possible (MediaWiki's PHP code is slow)

The other goal is to use that parser to extract various information from
the dump that is suitable for further processing and is stored in files
with simple syntax.


1.1 Relation to the original Wikiprep

Wikiprep was initialy developed by Evgeniy Gabrilovich to aid his research.
Tomaz Solc adapted his script for use in semantic information extraction
from Wikipedia as part of Zemanta web service.

This version of Wikiprep undergone some extensive modification to be able
to extract information needed by Zemanta's engine.


1.2 Current status

Currently implemented MediaWiki functionality:

  o Templates:
  
      - Named and positional parameters,
      - parameter defaults,
      - recursive inclusion to some degree - infinite recursion
        breaking is currently implemented in a way uncompatible with
	MediaWiki and
      - support for <noinclude>, <includeonly> and similar syntax.

  o Parser functions (currently only #if, #ifeq, #language)

  o Magic words (currently only urlencode, PAGENAME)

  o Redirects

  o Internal and external links

  o Proper handling of <nowiki> and other pseudo-HTML tags

  o Disambiguation page recognition and special parsing 

  o MediaWiki compatible date handling

  o Stub page recognition

  o Table and math syntax blocks are recognized and removed from the final
    output

  o Related article identification

Processing of an English Wikipedia dump on an up-to-date machine takes
approximately 50 hours.



2. Requirements
===============


2.1 Software

You need a recent version of Perl 5 with the following modules installed (names
in parentheses are names of respective Debian packages):

Parse::MediaWikiDump  (libparse-mediawikidump-perl)
Regexp::Common        (libregexp-common-perl)

You will also need gzip installed if you want to use the -compress option.



2.2 Hardware

English Wikipedia is big. Don't underestimate it. 

As of April 2008, Wikiprep output takes approximately 15 GB of hard disk
space (with -compress and without -log option). Debug log takes 20 GB or
more.

2 GB RAM is minimum, but 4 GB or more is needed. Wikiprep is currently
single threaded, so use the fastest CPU you can get. On a 2.33 GHz Intel
Xeon CPU it takes approximately 60 hours to process the English Wikipedia.



3. Usage
========

Run the following to get a list of available options:

$ perl wikiprep.pl

To run regression tests included in the distribution, run :

$ make test



4. Tools
========

There are a couple of tools in the tools/ directory that aim to make your
life a bit easier. Their use should be pretty much obvious from the help
message they return if you run them without any command line arguments.

  o findtemplate.sh

    Find templates that support a named parameter. Good for searching for
    all templates that support for example the "isbn" parameter.

  o getpage.py

    Uses Special:Export feature of Wikipedia to download a single article
    and all templates it depends on. It then constructs a file resembling
    a MediaWiki XML dump, containing only these pages.

    Good for making testing datasets or researching why a specific page
    failed to parse properly.

  o samplewiki

    Takes a complete MediaWiki XML dump, takes a random sample of pages
    from it and creates a new (smaller) dump.

  o riffle

    Takes a complete MediaWiki XML dump and some articles downloaded using the
    Special:Export feature and inserts these articles into the dump.

    Good for keeping an XML dump up-to-date without having to re-download the
    whole thing.



5. License
==========

Copyright (C) 2007 Evgeniy Gabrilovich (gabr@cs.technion.ac.il)
Copyright (C) 2008 Tomaz Solc (tomaz.solc@tablix.org)

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 2 of the License, or
  (at your option) any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA,
  or see <http://www.gnu.org/licenses/> and
  <http://www.fsf.org/licensing/licenses/info/GPLv2.html>


Some of the example files are copied from English Wikipedia and are 
copyright (C) by their respective authors. Text is available under the 
terms of the GNU Free Documentation License



6. Hacking
==========

FIXME
